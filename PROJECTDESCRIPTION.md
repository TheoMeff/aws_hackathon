Voice-Enabled FHIR Search System with Amazon Nova Sonic

Introduction

Healthcare providers often need quick, hands-free access to patient records and insights. This project envisions a voice-driven FHIR search system for doctors, enabling natural spoken queries and responses. It leverages Amazon Nova Sonic – AWS’s new speech-to-speech foundation model – to allow real-time conversational interactions with electronic health records. Instead of typing queries, a doctor can simply ask (e.g., “What were John Smith’s latest lab results?”) and hear an immediate answer. The system is built on the HL7 FHIR (Fast Healthcare Interoperability Resources) standard for medical data, ensuring interoperability with existing health record systems. By unifying speech recognition, language understanding, and speech generation in one model, Nova Sonic provides low-latency, human-like voice interactions ￼. It eliminates the need for separate transcription and TTS services ￼, preserving nuances like tone and speaking style for more natural conversations. Importantly, Nova Sonic supports function calling (aka tool use) and agent workflows, so it can query external data or perform actions during dialogue ￼. We will harness this capability via Bedrock Agents, allowing the AI to fetch patient data from a FHIR database in real time. The end result will be a web-based voice assistant for doctors that is intuitive, fast, and securely integrated with patient health records.

Key Features
	•	Natural Voice Interface – Doctors interact by voice. The application uses Nova Sonic’s bidirectional streaming API to transcribe the doctor’s speech and generate an immediate spoken response ￼, enabling back-and-forth conversation with virtually no delay. This voice-first design lets providers get information without interrupting their workflow or using their hands.
	•	FHIR Medical Record Search – Supports rich queries on patient records using the FHIR standard. A doctor can find a patient by name, date of birth, or ID, and retrieve all relevant information. The system can pull:
	•	Patient Demographics – Search for patients by name, DOB, MRN or other identifiers, and fetch patient details.
	•	Observations & Vitals – Retrieve vital signs (blood pressure, heart rate, etc.) and lab results for a patient.
	•	Conditions & Allergies – Get the patient’s active conditions, diagnoses, and recorded allergies.
	•	Medications & History – List current medications and medication history for the patient.
	•	Encounters & Procedures – Review past encounters (visits) and procedures the patient has had.
	•	Care Team & Plans – Identify the care team members and care plans associated with the patient.
	•	Essentially, any FHIR resource type (Patient, Observation, Condition, Medication, Encounter, Procedure, CarePlan, CareTeam, etc.) can be queried via voice. The assistant will handle the appropriate FHIR queries under the hood (including composite queries like FHIR’s $everything to get a patient’s full record if needed ￼).
	•	Semantic Search & Understanding – Beyond exact matches, the system provides semantic search capabilities. Nova Sonic’s integrated language model allows it to understand natural language queries (e.g. “Does John have high blood pressure?”) and map them to the right FHIR data (e.g. a Condition of hypertension). We will enhance this with a Bedrock Knowledge Base for Retrieval-Augmented Generation: the patient data can be embedded and indexed so the agent can retrieve relevant textual context when needed ￼. This means the assistant can handle colloquial or complex queries, using vector similarity to find relevant information even if the exact keywords aren’t present. The Bedrock Knowledge Base uses Amazon OpenSearch under the hood as a vector store ￼, enabling robust semantic search over unstructured notes or descriptions. In short, the doctor can ask questions in plain language and the system will comprehend intent and fetch the appropriate records, even for concept-based queries (e.g. “show recent cardiac tests” could find relevant EKG Observation entries).
	•	Web-Based User Interface (Static SPA) – The interface is delivered as a web application that doctors can load in a browser on a tablet or computer. The UI will be a clean, simple dashboard with:
	•	A microphone button to start/stop voice capture for queries.
	•	Real-time visual transcription of the doctor’s question as it’s being understood (for confirmation).
	•	A text display of the AI’s answer (e.g. listing the patient data or summary) alongside the spoken response.
	•	Optionally, UI elements to refine or filter results (for example, after asking for lab results, the UI could show a list of recent labs with dates).
The web app will be implemented as a static single-page application (HTML/JS, possibly using a framework like React). Being static, it can be easily hosted on Amazon S3 + CloudFront for scalability and low cost. All dynamic functionality (voice streaming, data fetching) is handled via JavaScript calls to backend APIs – no server-side pages are needed, aligning with a static hosting approach.
	•	Real-Time Streaming and Low Latency – The system architecture emphasizes real-time performance. Audio from the user is streamed to the backend and into the Nova Sonic model, which transcribes and generates the answer on the fly. Nova Sonic’s unified approach avoids the multi-step latency of separate services ￼, enabling near-instant responses. The voice conversation will feel natural and interactive, with Nova Sonic even adjusting its speaking style based on the user’s tone or pace for more fluid turn-taking ￼ ￼. This is crucial in a clinical setting where time is critical and any delay can be disruptive.
	•	Data Storage and Security – All medical data is stored and accessed in a secure, HIPAA-compliant manner. We will use AWS HealthLake as the managed FHIR data store (or an equivalent FHIR server) to house patient records. HealthLake natively supports FHIR R4 and indexes data upon ingestion, making it easily searchable by standard FHIR queries ￼. Under the hood, HealthLake uses Amazon S3 for durable storage of the JSON resources and automatically indexes them for fast querying ￼. This provides a scalable repository for our data, with compliance features (encryption, audit logs) out-of-the-box. The voice assistant will access patient data via the HealthLake FHIR API (e.g. REST calls like GET /Patient?name=Smith or complex searches with filters). This ensures no PHI leaves the secure AWS environment – the Bedrock model’s queries to data happen via secure API calls, and only relevant snippets or summaries are returned to the user. We will also integrate AWS Identity and Access Management (IAM) and possibly Amazon Cognito for user authentication, so that only authorized clinicians can access patient data through the app. Each query can be logged for audit purposes (who accessed what record and when), to maintain compliance.

(Yes, the system will use multiple AWS services – Amazon Bedrock (Nova Sonic model + Agents), HealthLake (FHIR store), Amazon S3 (for web hosting and any auxiliary data), and others as needed – to create the most compelling and secure product.)

System Architecture

Overall Architecture: The solution can be viewed in two parts – the front-end (voice-enabled web client) and the back-end (AI and data services on AWS). Below is a high-level workflow of a user query:
	1.	Voice Capture (Client): On the web UI, the doctor presses a “Talk” button and asks a question. The browser uses the Web Audio API to capture the microphone input and streams the audio data (likely as 16 kHz PCM or Opus) to the back-end service over a WebSocket or HTTP/2 stream.
	2.	Nova Sonic Processing (Bedrock): The audio stream is fed into the Amazon Bedrock service where the Nova Sonic model processes it in real time. Nova Sonic transcribes the speech to text and interprets the user’s intent using its built-in LLM capabilities. Crucially, we configure Nova Sonic with a system prompt and Bedrock Agent so that it knows how to handle queries. For example, the system prompt might instruct: “You are a medical records assistant. When the user asks about patient data, you should retrieve information from the FHIR database and then respond with the relevant details. Use concise, clinical language.” This guides the model’s behavior. The Bedrock Agent mechanism will let Nova Sonic decide if it needs to invoke an external action (such as searching the database) to fulfill the request ￼.
	3.	Agent & Tool Use (Orchestration): If the query requires fetching data (almost always in our use case), Nova Sonic will utilize Agents for Amazon Bedrock to perform the necessary calls. We will define a set of tools (APIs) that the agent can use, corresponding to FHIR data queries. For example, we can create an Action Group with actions like FindPatient(name) or GetObservations(patientId) etc., or even direct FHIR REST endpoints. The agent can be configured via an OpenAPI schema or Lambda functions to call the HealthLake FHIR API. For instance, if the doctor asks “What medications is Jane Doe on?”, the agent might call the GET /MedicationRequest?patient=12345 endpoint. Bedrock Agents use a ReAct (reasoning and action) loop ￼: Nova Sonic will reason about the query, decide which action to call (e.g., a database lookup), Bedrock executes that call, and the result (observation) is fed back into the model’s context. This all happens behind the scenes in a fraction of a second. Using Agents, the model can orchestrate multi-step tasks and integrate external knowledge seamlessly ￼. The knowledge base integration means the agent can also perform semantic RAG searches if needed – for example, searching within unstructured notes for relevant info – by querying the vector index of a Bedrock Knowledge Base ￼.
	4.	FHIR Data Retrieval (HealthLake): The back-end FHIR store (AWS HealthLake) processes any queries from the agent. HealthLake is optimized for such queries – it automatically indexed the data by patient, dates, codes, etc., so queries like “all vitals for patient X” or “active conditions for patient X” are efficient ￼. The response from HealthLake (in FHIR JSON format) is returned to the agent’s action. For example, a query for vitals might return a bundle of Observation resources. If the result is large or needs post-processing, a Lambda function could summarize or filter it (e.g., pick the latest value). This data is then given back to Nova Sonic as context. We may also map the raw FHIR JSON into a more conversational format – for instance, converting a coded diagnosis into a patient-friendly term (the system could leverage the terminology in the FHIR resource or a small translation dictionary for medical codes).
	5.	AI Response Generation: Now Nova Sonic has both the user’s question and the retrieved data (embedded in its prompt). It will compose a spoken answer. Because Nova Sonic is a speech-to-speech model, it directly generates voice output in a natural-sounding voice, rather than just text. For example, after retrieving John Smith’s blood pressure readings, Nova Sonic might respond in speech: “John Smith’s latest blood pressure was 130 over 85 on March 10th, which is slightly elevated. His prior reading on Feb 5th was 128 over 80.” The response content is derived from the live data but phrased in a concise way by the LLM. (We will enforce that the model uses the data and not any hallucinated info by grounding it with the knowledge base results. The use of Bedrock Knowledge Bases and tools ensures the model’s answers stay factual ￼.) The audio output is streamed back to the client as it is generated. Nova Sonic’s streaming API will send chunks of audio (and we can also get the text transcript of the output if needed).
	6.	Client Playback and Display: The web client receives the audio stream and plays it to the user through the speakers. The doctor hears the answer spoken naturally. Simultaneously, the app can display the answer text and any structured data (like a table of values, list of medications, etc.) on screen for reference. This dual output (voice + visual) caters to different preferences and allows the doctor to read details if needed. The UI can also highlight or provide follow-up options – e.g., “Would you like to see more details?” or enable the doctor to ask a follow-up question continuing the context (the conversation history is maintained in the Nova Sonic session context, so the doctor could just say “What about her allergies?” as a follow-up, and the model will understand it refers to the same patient).

Components: To summarize, the architecture consists of the following major components:
	•	Amazon Nova Sonic (Bedrock) – The core AI model providing speech-to-speech capabilities. It transcribes incoming speech, understands intent, and generates spoken responses. Nova Sonic’s unified model means our architecture can be simpler (no separate Amazon Transcribe or Polly needed) and latency is minimized ￼. It also brings advanced features like adaptive responses (adjusting to user emotion/tone) and interruption handling (the model can gracefully handle if the user barges in or asks a new question mid-response). Nova Sonic is configured with a system prompt (defining its role and style – e.g. “helpful medical assistant”) and integrated into a Bedrock Agent that gives it access to our tools and knowledge base.
	•	Bedrock Agent and Tools – The agent layer that enables function calling and knowledge grounding for the model. We will create an Agent in Amazon Bedrock configured with:
	•	Action Groups (APIs): These define how the agent can query the FHIR data. We will either use the HealthLake FHIR endpoints directly (Bedrock could call AWS APIs if permissions and schemas are set) or, more likely, expose a set of REST APIs via API Gateway/Lambda that wrap the FHIR queries. For example, a Lambda function findPatient(name) could internally call HealthLake.searchPatients(name) and return a list of matching patient IDs/names. The agent will have the OpenAPI specs for these actions, so it knows when and how to call them. Using Bedrock’s tool-use capability, the model can decide at runtime which action to invoke based on the question. This approach is aligned with Amazon’s guidance that Agents can orchestrate multi-step tasks by calling external APIs ￼.
	•	Knowledge Base: We connect an Amazon Bedrock Knowledge Base to the agent, populated with domain-specific content. In our case, “domain content” could include medical terminology definitions, care guidelines, or even the patient data itself for semantic search. However, uploading all patient data into the knowledge base may not be ideal for per-patient queries (it’s better to call the database). Instead, a useful approach is to populate the knowledge base with reference documents: e.g., clinical guidelines for interpreting vitals, or definitions of conditions. The agent can then use RAG to pull in explanatory info if a question requires it (for instance, if a doctor asks “What does this lab result mean?” the model could retrieve a snippet explaining that lab). The knowledge base could also store unstructured notes or documents that are not easily stored as FHIR resources, if needed. By linking the knowledge base to an S3 bucket of documents, Bedrock will index them and enable semantic retrieval ￼ which the agent can utilize.
	•	Agent Workflow: The Bedrock Agent uses an Orchestration Prompt Template behind the scenes to decide the sequence of actions (using a ReAct style reasoning ￼). For example, it might reason: “User asked for X, I should call the FindPatient action, then use that result to call GetObservation, then respond with the summary.” These steps and the observations (results) are fed into the model prompt before final answer generation ￼ ￼. We will test and fine-tune the agent configuration to ensure it reliably uses the tools and includes the retrieved data in answers. This might involve adjusting prompt instructions or using Bedrock Guardrails to prevent the model from veering off (e.g., a guardrail can enforce that no patient data leaves or that the model doesn’t answer from memory if a tool should be used).
	•	FHIR Data Store – The repository of medical records. During development and testing, we can use a local SQLite database with synthetic FHIR data (as mentioned in the original setup, a data/ folder to hold a SQLite DB file). The schema might be a simple table of resources as given (id, resource_type, content JSON) to simulate a FHIR server. However, for a robust product, we will use Amazon HealthLake, which is a fully managed HIPAA-eligible FHIR data store. HealthLake allows ingestion of bulk FHIR data (e.g., from EHR systems or CSVs) and automatically normalizes and indexes it ￼ ￼. It supports the full FHIR REST API for queries ￼, meaning we can leverage its search functionality rather than writing our own SQL. HealthLake stores the data in S3 and keeps an index updated so that queries are fast ￼. Using HealthLake offers immediate support for all standard FHIR resource types (Patients, Observations, Conditions, Medications, etc. – over 70 resource types ￼) and FHIR search parameters. This drastically reduces development effort as we don’t need to implement search logic – we just call the API. Another benefit is that HealthLake has built-in PHI text extraction and normalization (using Comprehend Medical under the hood) which enriches the data. For example, if there are clinical notes in a DocumentReference, HealthLake can automatically extract medical entities and link them to standardized codes ￼. Such enriched data can improve our semantic search results. In summary, the FHIR store (be it SQLite for local dev or HealthLake in AWS) is a critical component that the agent queries to get the ground-truth patient data.
	•	Backend API & Stream Manager – We will implement a backend service (likely in Python, given the Bedrock SDK support and our preference for Python 3.10+ environment). This service has two main responsibilities: audio streaming and agent invocation. It will expose an endpoint (e.g., a WebSocket or an HTTP/2 stream endpoint) that the web UI can connect to for streaming audio. When the client sends audio, the backend forwards it to the Bedrock InvokeModelWithBidirectionalStream API for Nova Sonic ￼. We maintain the streaming session until the query/response cycle is complete. This backend will handle events from Bedrock as well – as Nova Sonic emits the transcribed text and partial audio for the response, the backend will relay those to the client in real time. Essentially, the backend acts as a bridge between the browser and Amazon Bedrock. It also needs to handle Bedrock Agent function calls: depending on Bedrock’s implementation, the agent might call our actions via direct HTTPS calls (if we register APIs with it), or it may return a payload that the SDK has to interpret and then execute. We will likely use the Bedrock Agents API/SDK which can manage executing the Action Groups. If using custom Lambdas for data retrieval, our backend could also directly call those Lambdas (or the HealthLake API) when instructed by the agent. Security-wise, this service will run with AWS credentials that allow it to invoke Bedrock and read from the HealthLake data store (via IAM roles). We will deploy this component on AWS – possible choices include AWS Lambda (if we can make it work with streaming, perhaps via WebSocket API Gateway) or an Amazon EC2/Fargate container that keeps a persistent connection. A simple approach is an EC2 instance running our Python server code (maybe using FastAPI or Flask + Socket.IO for WebSocket). For scalability, a container on ECS Fargate behind an Application Load Balancer might be used for production. This backend will also be responsible for any necessary data processing (e.g., formatting the FHIR JSON into a summary).
	•	Web Front-End – A static web application that provides the user interface. We will develop this as a single-page app (SPA) with modern JavaScript. Key aspects of the front-end implementation:
	•	Use the Web Audio API or MediaRecorder to capture microphone input. We may encode audio in a suitable format (Nova Sonic expects 16 kHz PCM audio frames; we might use PCM or Opus in an Ogg container for efficiency). The Amazon Nova Sonic workshop and samples provide guidance on audio handling; likely we will use an existing JS audio stream library to send small audio chunks continuously to the backend.
	•	Establish a WebSocket connection to our backend. As the user speaks, audio chunks are sent over the socket. We’ll also listen on the socket for incoming messages, which will include partial transcripts and the synthesized audio bytes of the answer.
	•	Play the incoming audio stream. The front-end will reconstruct the audio stream (if in chunks) and output through an <audio> element or Web Audio API for low-latency playback. This way, the doctor can hear the answer as it’s being generated (Nova Sonic supports streaming out, so the voice will start speaking before the entire answer is finished – improving responsiveness).
	•	Display text: Show the interim transcription of the user’s query (this helps the user verify the system understood them correctly). Once the agent formulates an answer, display the answer text as well. If the answer includes structured data (like a list of medications), the UI can format that in a readable way (table or bullet list). We can even highlight important values (e.g., flag high lab results in red).
	•	Provide controls: Start/stop recording button, and maybe fallback text input (in case a doctor is in a noisy environment and prefers to type, we can allow textual queries as well – the backend can handle text queries by skipping Nova Sonic and using a text LLM or directly querying, but this is optional).
	•	The web app will be built for simplicity and speed (probably plain HTML/JS or a lightweight framework). After building, we will deploy it on Amazon S3 as a static website and use Amazon CloudFront CDN to ensure low-latency access from clinics anywhere. Hosting as a static site means no server needed for the UI, and updates are as simple as uploading new files to S3.

All these components work together to deliver a seamless experience: the doctor feels like they are conversing with a smart assistant that has instant access to the patient’s chart. The use of AWS managed services (Bedrock and HealthLake) ensures we have a scalable, secure backbone without reinventing core functionality.

Implementation Plan

Following is a step-by-step plan to implement the project, from setup to deployment:

1. Environment Setup and Prerequisites
	•	AWS Account & Permissions: Ensure you have an AWS account with access to Amazon Bedrock (Nova Sonic) – this is still a relatively new service (as of 2025) and may require opting in or enablement in the AWS Console. Enable Amazon Nova Sonic model access in Bedrock ￼ ￼. Also, create or identify an IAM role with permissions for Bedrock (InvokeModel), HealthLake (read access to the datastore), and any other services (S3, etc.). If using Agents, you may need to allow the Bedrock service to assume certain roles to execute actions (per documentation).
	•	Python Environment: Set up a Python 3.10+ environment for development. We recommend using a virtual environment (the instructions provided suggest using uv (uvenv) or you can use venv/conda). For example:

mkdir fhir-voice-search && cd fhir-voice-search  
python3 -m venv .venv  
source .venv/bin/activate  
pip install -U pip  


	•	Backend Dependencies: Install required Python packages. These will include the AWS SDK (boto3 or the newer botocore/bedrock SDK for Bedrock), possibly the AWS CLI for testing, and libraries for WebSockets or HTTP/2. For streaming, AWS provides gRPC/protobuf for the Bedrock streaming API; we might use the AWS SDK’s high-level interface if available. Also, if we use FastAPI or Flask for the web socket server, include those. We will maintain a requirements.txt – likely including boto3, aws-bedrock-runtime (if exists), fastapi, uvicorn (for ASGI server), websockets or similar.
	•	Front-end Tools: Set up Node.js if needed for front-end build (e.g., if using a framework like React, or to use build tools). Or you can do without a heavy build and just write vanilla JS with perhaps a bundler for convenience. If using a framework, initialize it (create-react-app or Vite, etc.).

2. Project Structure
Create the project directories for organization. For example:

fhir-voice-search/  
├── backend/  
│   ├── server.py           # backend server code  
│   ├── agent-config/       # JSON/YAML for Bedrock Agent (if any)  
│   └── requirements.txt    # backend Python deps  
├── web-ui/  
│   ├── index.html          # main HTML  
│   ├── app.js             # JS logic for UI  
│   └── style.css           # styling if needed  
├── data/  
│   └── fhir.db             # SQLite database (for local dev mode)  
└── README.md  

The data/ folder will hold any local data files (like the SQLite DB or sample FHIR JSON files). If using HealthLake, we might not need data/ beyond maybe some test data to import. For development, you can export a subset of FHIR resources (e.g., in JSON) and place in data/ for initial tests. Also prepare a .env file (in the root or backend/) to store configuration like AWS region, HealthLake Data Store ID, and API endpoints.

3. FHIR Data Store Setup
	•	Option A: Using AWS HealthLake: Create a HealthLake Data Store via the AWS Console or CLI. Choose the FHIR version (R4) and give the datastore a name (e.g., “DoctorAssistData”). It will take some time to initialize. Once ready, you can load data. If you have existing FHIR data (e.g., from an EHR export or synthetic dataset), use the HealthLake import feature or the FHIR API to create resources. AWS offers a bulk import where you upload NDJSON files to S3 and start an import job. Alternatively, for testing, manually POST a few Patient and related resources using the AWS CLI or Postman to the HealthLake endpoint. Ensure the IAM role for Bedrock (or the one your backend will use) has permissions to read HealthLake (which is typically done via AWSLakeFormation or specific ARNs).
	•	Option B: Using Local SQLite: If HealthLake is not immediately available, implement a simple local store. Use the schema provided: a resources table with columns (id, resource_type, content JSON, created_at). You can write a small script to ingest sample data (perhaps generate a few Patients and Observations). This local DB can be accessed via an ORM or direct SQL in Python. It won’t support the full FHIR query language, but for basic testing you can retrieve by patient name or ID with SQL LIKE queries on the JSON content (or store a separate table for patient names to index them).
	•	Semantic Data Prep: If using a Knowledge Base for semantic search, prepare the data source. For example, create an S3 bucket to hold reference docs or a CSV of medical terms. One idea is to compile a list of FHIR Code -> Layman term mappings or brief explanations of lab results, and put that in a document the KB can ingest. Connect this data source to a Bedrock Knowledge Base via the console, and sync it ￼. This will allow Nova Sonic to pull context that isn’t directly in the structured data, if needed for more conversational answers. (This step is optional but enhances the assistant’s usefulness by allowing brief explanations or related info.)

4. Configuring the Bedrock Agent
This is a crucial step to enable Nova Sonic to use our tools. We will do the following:
	•	Define Action Group(s): Create an OpenAPI specification (JSON) that defines the actions the agent can take. For example, define a “FHIRSearchAPI” with endpoints like:

/patients/search:  
  get:  
    summary: Search patients by name  
    parameters: [{ name: "name", in: "query", required: true, ... }]  
    responses: { 200: { description: "OK", content: {... FHIR Patient bundle...} } }  
/patients/{id}/observations:  
  get: ...  (to get Observation list for patient)  

We can map these endpoints to actual calls in our backend. In practice, the OpenAPI file will be uploaded to S3 and referenced in the Agent configuration. Each path can correspond to a Lambda function or an API Gateway endpoint. For initial simplicity, we might implement these as local functions callable in our Python backend. But the Bedrock Agent expects an HTTPS endpoint to call if it’s fully automated, so we might stand up a small API (maybe our same server.py could have routes for these). For now, plan that we have an API endpoint for each needed action. List out all the needed actions (likely aligned to the features list: find patient, get observations, get meds, etc.).

	•	Create the Agent: Using the AWS SDK or console, create a new Bedrock Agent. In the configuration, specify:
	•	The foundation model: amazon.nova-sonic-v1:0 (Nova Sonic version 1).
	•	The agent instructions (system prompt): Here we input a carefully crafted instruction set for the model, covering how it should behave. For example: “You are a voice assistant for doctors, with access to patient records via FHIR. Only use the provided tools to get information – do not make up data. When giving answers, be concise and factual. If multiple results are found, summarize briefly. Speak in a calm, professional tone.” We will refine this prompt through testing. It should encourage tool use by saying something like “If a question requires patient data, use the available actions to retrieve it.” Nova Sonic supports such system prompts to maintain role and style ￼ ￼.
	•	The action groups: attach the OpenAPI action group we created. This tells the agent what tools are available.
	•	The knowledge base: attach the Knowledge Base ID if we set one up for reference docs.
	•	Optional: define guardrails/policies if needed (Bedrock allows some predefined guardrails for content filtering or response formatting). In healthcare, we might enable a guardrail that prevents it from giving advice outside of provided data or to ensure no PHI is output to unauthorized channels (though since user is authorized, maybe not needed).
	•	Test Agent in Console: Before integrating with our code, it’s wise to test the agent in the Bedrock console or via the AWS CLI. We can do a test text prompt like: “Find patient John Smith born 1980 and list his conditions.” The agent should ideally respond after internally calling the tools (the console might show the chain of actions taken). If it returns the correct info (or at least attempts to call an action), we know the configuration is correct. We may need to adjust the prompt or the action definitions if the agent isn’t picking the right action. For instance, we might add few-shot examples in the prompt, like: “User: Show me Alice’s medications. Assistant (thinking): I should call the medications API… Assistant (action): GET /patients/123/medications … Assistant: Alice is taking metformin and lisinopril.” – This can help the model understand how to use the tools.

5. Backend Development
Now implement the backend server that ties everything together:
	•	Streaming Handler: Using Python asyncio or threads, set up the code to call bedrock-runtime streaming API. The AWS documentation and the Nova Sonic Medium article provide sample code on using the InvokeModelWithBidirectionalStream ￼. Essentially, you initialize a Bedrock runtime client with the model ID and open a streaming session. You send a “model prompt” event to start the session (this attaches the agent, system prompt, etc.), then continuously send audio chunks from the user as they arrive. Nova Sonic will emit events for transcripts and for the final audio. We have to read from the stream in parallel. This is naturally handled with asynchronous generators or callback hooks. We will wrap this in an API on our server. For example, in server.py, we might use FastAPI with WebSockets: when a client connects and sends an audio frame, our code writes it into the Bedrock stream; when Bedrock yields a response event (like “transcript: …” or “audio_out: …”), we forward that to the WebSocket. This is the most complex part, dealing with concurrent streaming I/O, but we can follow patterns from AWS’s sample (if available). The Medium post by Sidharth Rampally shows how he built a simple Nova Sonic class with an audio queue in Python ￼, which we can adapt. We’ll ensure to handle session closure properly (after a query is answered, if the user is done speaking or says “stop”, we end the Bedrock stream and close the socket).
	•	Tool Invocation: If we set up Bedrock Agent to auto-call HTTPS endpoints for tools, our backend must host those endpoints. For example, if the agent tries to GET /patients/search?name=John, it should reach our server. We can implement an HTTP GET route for /patients/search that reads the query param, then queries the HealthLake FHIR API (using boto3 for HealthLake or just requests to the HTTPS endpoint if easier). The result (FHIR bundle JSON) is returned. The Bedrock agent will receive that JSON as the action result. We might need to format it or limit it (maybe just return the first match or summary if multiple). Similarly, implement routes for each action (observations, etc.). These routes essentially translate the agent’s intent to the real data. Since the agent runs within Bedrock, it will call our API likely using the Bedrock service role. We must ensure our API is accessible (if we run the backend on localhost for dev, we may not be able to expose it publicly; for full integration test, deploying the backend on an EC2 with a public URL or using something like ngrok for a quick test might be required). In a production scenario, we’d have these behind API Gateway with authentication (to ensure only Bedrock or authorized calls hit them). For now, in development, we can simulate the tool call by having the agent just output what call it would make, and we intercept that. Alternatively, a simpler route: inline function calls – since we control the streaming process, we could capture the model’s intent to call a function via the transcript events. But Bedrock Agents likely handle it internally. We will follow the official method of hosting an endpoint and letting the agent call it.
	•	HealthLake API Calls: Use the AWS SDK for HealthLake to search data. For example, boto3.client('healthlake').search_datastore(...) or use the FHIR-standard REST. HealthLake’s endpoint for search might be like: https://healthlake.<region>.amazonaws.com/datastore/<id>/r4/Patient?name=John. We will likely need to use SigV4 signing or the AWS SDK to call it securely (the agent’s role must have permission). Since our backend is running with credentials, we can just use requests with auth. A tip: HealthLake search by name may require exact matches unless using wildcards; we might implement partial matching by trying multiple queries (e.g., try family name, etc., or retrieve all and filter – but that’s inefficient if data is large). We can refine search later. The key is we can retrieve patient ID from name, then use that ID for other queries. We’ll implement helper functions in our backend for each type of query to keep things organized.
	•	Testing Backend Locally: We will write unit tests or simple scripts to simulate a voice query. One approach is to bypass audio for initial testing: call the backend’s logic with a text query. For instance, we might temporarily use a text LLM (like Amazon Titan or Claude on Bedrock) with the same agent to test the prompt and tool execution in a non-voice mode. Or use Nova Sonic with a recorded audio file of someone asking a question. This can be done by reading an audio file and sending it into the stream (the Medium article shows usage with microphone live input; we can adapt it to file input for testing). Verify that the end-to-end flow returns the correct data. If issues arise in the agent’s usage of tools, iterate on the agent config. This step is iterative – adjusting prompts, verifying the backend is receiving tool calls, ensuring the data returned is correct, etc.

6. Front-End Development
With the backend functional, we build the web UI:
	•	Create a basic HTML layout: one section for the conversation (transcript and response) and a microphone control. For example:

<div id="conversation">
  <div id="transcript" class="user-query"></div>
  <div id="response" class="assistant-answer"></div>
</div>
<button id="micButton">🎤 Tap to Speak</button>

Add some CSS to position the elements nicely (maybe the button at bottom, conversation text above). Visual cues: when recording, the mic button could turn red, etc.

	•	Write app.js: This will handle connecting to the backend and managing audio. Pseudocode flow:

const socket = new WebSocket(BACKEND_WS_URL);
socket.onopen = () => console.log("Connected to backend");
socket.onmessage = (event) => {
    // handle incoming messages (which could be JSON distinguishing transcript vs audio)
    let data = event.data;
    if (data.type === 'transcript') {
        document.getElementById('transcript').innerText = data.text;
    } else if (data.type === 'audio') {
        audioPlayer.playChunk(data.audioChunk);
    } else if (data.type === 'final') {
        // final response text
        document.getElementById('response').innerText = data.text;
    }
};

We need an audioPlayer that can handle incremental audio. Possibly we can use the Web Audio API: create a MediaSource and append buffers as they arrive. Another simpler way: the backend could encode audio to base64 WAV data and send complete or partial WAVs we can just play with an <audio> tag source that updates. However, streaming small chunks smoothly is tricky. A known approach is to use the Web Audio API by decoding raw PCM and pushing to an AudioBufferSourceNode. We can handle that with AudioContext.decodeAudioData if we have proper WAV/OGG framing. This is a technical detail that will require some experimentation. For the scope of the plan, suffice to say we will implement a continuous audio playback mechanism for the stream. (Nova Sonic outputs 24 kHz 16-bit mono by default ￼, we should ensure to play at that sample rate or convert to 48 kHz for some browsers – minor detail.)

	•	Implement the recording functionality: Use MediaRecorder if supported for real-time streaming (or AudioContext + ScriptProcessorNode). We aim to get small chunks (say 0.5 second of audio or less) and send them as soon as possible. With MediaRecorder, we can set timeslice to, e.g., 250ms and ondataavailable send that blob to the socket. Alternatively, use an AudioWorklet for more control. The simplest might be:

let mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
let recorder = new MediaRecorder(mediaStream, { mimeType: 'audio/webm;codecs=pcm' });
recorder.ondataavailable = (e) => socket.send(e.data);
recorder.start(250); // send blobs every 250ms

The backend would then need to decode WebM/PCM to raw PCM frames for Nova Sonic. This adds complexity; we might choose audio/wav or raw PCM in the media recorder if possible. Alternatively, use Opus and decode server side. Given time, using PCM frames directly might be easier: an AudioWorklet could give raw PCM samples which we then package (but that’s advanced). We may rely on the Nova Sonic sample code which likely handled audio capture – possibly they recorded audio via Python and forwarded it. Since we want all in browser, we’ll experiment. (The Nova Sonic dev.to blog indicates they ran a local Python server that accepted audio – we’ll essentially do the same but initiated from browser.)

	•	Provide user feedback on UI: e.g., when the assistant is speaking, maybe show a loading spinner or a waveform animation. Also, handle the mic toggle – clicking the mic starts recording and sends an initiation signal to backend to start a new Bedrock streaming session. If the user clicks again (or after an answer), we may end that session. We should support continuous conversation, but initial version can be one question at a time.
	•	Testing the UI: Run the front-end (if using a dev server or just open the HTML file if CORS to backend is allowed). Use a test patient in the system to try queries. For example, speak “Find patient John Smith born 1975.” See that the transcript appears, and the response comes back (perhaps “I found patient John Smith, ID 12345. What would you like to know about him?” if we program a follow-up). Then ask “What are his latest vitals?” and see if those populate. We will likely debug issues like audio quality, timing (maybe the first few words might be cut if streaming not set up before talking – we may open the socket and stream silence or a start event to Bedrock to ensure it’s ready). We refine as needed.
	•	Optimize the UI for static hosting: That means ensure any API URLs are relative or configurable (for easy deployment), and the app does not require a server. We’ll build/compile if using a framework, then test opening the index.html from disk or via an S3 static site to confirm it works with the backend endpoint.

7. Iteration – Fine-tuning the Voice Experience
Once the basic functionality works, spend time improving the “doctor experience”:
	•	Response formatting: Ensure that when multiple data points are returned, the assistant’s spoken answer is organized. We might adjust the agent’s prompt to say “When listing multiple items (e.g., medications), enumerate them clearly.” This way, if a patient has 5 medications, the assistant might say “He is currently prescribed: 1) Metformin, 2) Lisinopril, 3) Lipitor…” instead of a run-on sentence. We could also incorporate SSML (Speech Synthesis Markup Language) in the response if Nova Sonic supports it, to add brief pauses or adjust pronunciation for medical terms. Nova Sonic might not need SSML as it’s an FM, but if needed, we could use it or simply trust the model’s natural speech.
	•	Handling Ambiguity: If the user asks for a patient not found, or a date that doesn’t exist, the system should handle gracefully. The agent can detect no results (e.g., our API returns no patient) and Nova Sonic can respond, “I’m sorry, I couldn’t find that patient. Could you repeat or check the spelling?” We will implement such logic: if a search returns zero, our API might return a specific message which the agent can incorporate. Bedrock Agents allow checking the action result; we could even raise an exception that the agent interprets. We may add to the prompt: “If a tool returns an error or empty result, apologize and ask for clarification.”
	•	Contextual follow-ups: Ensure that Nova Sonic keeps the conversation context. Bedrock’s streaming API likely maintains context until you reset the session. We will keep the session open for a series of related queries. For example, after identifying the current patient, if the doctor’s next question doesn’t specify the name again, the assistant should assume the same patient context. We can achieve this by storing the active patient ID in the agent’s memory (maybe as a variable in prompt or as part of conversation history: e.g., assistant might internally note “user is now asking about patient 12345”). This isn’t out-of-the-box, but we can have the agent set a “focus patient” after a successful lookup. Another strategy: use the conversation memory – once the assistant provided data about John, if next query is “Show me his labs”, the LLM will likely infer “his” = John. But to be safe, we can intercept such follow-ups and inject the patient ID into the tool call.
	•	Voice quality and voices: Nova Sonic’s default voice is a neutral AI voice. If we want a specific voice (male/female or a certain tone), Nova Sonic may allow some control or we could pipe the text output to Amazon Polly Neural voices. However, that would break the unified model benefit. Nova Sonic presumably generates its own voice output. Perhaps in future it will allow voice selection. For now, we’ll accept the default, which is described as expressive and capable of adjusting to input speaker’s prosody ￼. That likely means if the doctor sounds urgent, the assistant might respond in an upbeat tone. We will test how it sounds and note any adjustments.
	•	Performance scaling: With the voice streaming working, consider throughput. One user’s conversation is a single Bedrock streaming session. If multiple doctors use it simultaneously, our backend must handle multiple sessions in parallel. We should design the server to support that (i.e., not a single global session). Using an asynchronous framework (FastAPI with websockets or Node.js) can handle concurrent users. Bedrock will scale the model invocation as needed (with cost per use). We should also be mindful of Bedrock’s quotas – streaming sessions may have time limits or concurrency limits. We’ll consult AWS docs on Nova Sonic quotas. If needed, we might queue or limit sessions, but ideally it should scale to many simultaneous uses (the hospital might have dozens of doctors). We might use multiple backend instances behind a load balancer for high availability.
	•	Cost considerations: Each voice query costs Bedrock invocation and potentially HealthLake usage. Nova Sonic is priced per input/output token (in this case audio seconds) ￼. The Medium article estimated ~$7 for 10 hours of conversation ￼ – which is quite affordable for our use case (a doctor might use it a few minutes per patient). HealthLake has charges for storage and requests (but negligible per query). We will implement monitoring: perhaps integrate Amazon CloudWatch to log each query duration and maybe use CloudWatch Logs to store transcripts (if allowed by compliance, or at least for debugging in dev). Knowing usage can help optimize cost (e.g., if some queries are very long we might want to encourage briefer interactions).
	•	Security & Privacy Enhancements: Before production, ensure all data in transit is encrypted (WebSocket over wss with TLS, API calls with HTTPS). Use Cognito or a login for the web app so only authenticated users (doctors/nurses) can invoke the assistant. Also consider implementing Verified Permissions or similar to enforce that a doctor can only access patients under their care (this could be done by tagging data or having a patient list per user and checking before fulfilling the query). This might be beyond initial scope, but the architecture should accommodate such checks (maybe in the tool Lambda, verify user’s JWT from Cognito and cross-check patient permissions).
	•	Logging and Error Handling: Implement robust error handling: if the Bedrock stream fails or the agent errors out, inform the user (via a polite voice message “Sorry, something went wrong. Let’s try again.”). Also log errors for debugging. Because multiple systems are involved, add logging at each stage (client logs events, backend logs state changes, tool calls, etc.). This will help if, say, the agent doesn’t call a tool when expected – we can see the model’s raw output in logs and adjust prompts accordingly.

8. Deployment
With a tested solution, the final step is deployment in AWS for real use:
	•	Backend Deployment: Containerize the backend (e.g., a Dockerfile for the Python server). Then deploy on AWS Fargate (ECS) or as a Kubernetes pod if using EKS, or even as a simple EC2 instance managed via Systemd. AWS Fargate is appealing for a serverless approach – define a Task with our container, set it behind a Load Balancer for the WebSocket endpoint. Ensure the container’s task role has necessary IAM permissions (Bedrock Invoke, HealthLake read). Use AWS Parameter Store or Secrets Manager to store config like Bedrock endpoint, HealthLake datastore ID, etc., and load those as env variables. If using API Gateway WebSocket, you could deploy Lambda functions for each route, but managing the continuous stream through API Gateway might be more complex than a direct socket in our case. So likely an EC2/Fargate solution with an ALB that supports WebSockets (ALB does support WebSocket upgrade). We’ll open the necessary ports (443 for HTTPS).
	•	Domain and Certificate: If this is a customer-facing product (within a hospital network), we might configure a custom domain (like voiceassistant.hospital.com) for the web UI and API. Use AWS Certificate Manager for SSL cert and attach to CloudFront (for S3) and the ALB. This gives a nice URL for users.
	•	Web App Deployment: Upload the built web-ui files to an S3 bucket configured for static website or, better, just for origin (since we’ll use CloudFront). Configure a CloudFront distribution to serve these files globally, with an OAC (Origin Access Control) so the S3 isn’t public. The CloudFront will also provide HTTPS. We’ll set appropriate cache settings (the HTML maybe no-cache, JS/CSS can be longer cached). After deployment, test that the web app loads and is able to connect to the backend WebSocket (adjust the endpoint URL if needed to the production domain).
	•	Monitoring: Set up Amazon CloudWatch dashboards or X-Ray tracing to monitor the system. For example, track Bedrock Agent invocations, any errors from actions, latency of HealthLake queries, etc. We can use CloudWatch Logs in the backend container to aggregate logs. Also enable AWS Bedrock logs/metrics if available for Nova Sonic (to see number of tokens processed). Monitoring ensures we can catch issues (like if the agent fails frequently or if response times degrade).
	•	Analytics & Feedback: Optionally, gather usage metrics – e.g., count of queries per day, most common query types – to further improve the system. This could involve logging queries in a DynamoDB or an S3 log file (with PHI considerations – perhaps just log the categories of queries, not full details). This would help demonstrate the system’s value (e.g., “doctors retrieved 500 lab results via voice this week”).

9. Sample Use Case Walk-through (Example)
To illustrate the end result, consider the following scenario after deployment:
	•	Dr. Alice, in a patient exam room, opens the web app on her tablet. She sees a simple interface showing a microphone icon. She taps it and asks, “Find patient Emily Johnson, born in 1955.”
	•	The assistant (Nova Sonic) streams back: “Searching for Emily Johnson…” (It might say this if we program it to acknowledge queries). In a second, it finds a match and then says, “I found Emily Johnson, born July 12, 1955. How can I assist with her records?” (At this point the UI might also show basic info like patient ID and demographics pulled from the Patient resource.)
	•	Dr. Alice follows up: “What were her latest lab results?” (She doesn’t repeat the name; the system uses context that Emily Johnson is the subject.)
	•	The assistant uses the patient ID to query Observations of category “laboratory”. It then responds in voice: “Her most recent lab results include a cholesterol test on Sep 10, 2025: total cholesterol 180 mg/dL, HDL 55, LDL 100; and a glucose test on Sep 10, 2025: 90 mg/dL, which is normal.” The UI displays these values perhaps in a small table for clarity, and the text is shown as well.
	•	Dr. Alice then asks: “Has her blood pressure been high lately?”
	•	The assistant fetches vital sign Observations and perhaps also knowledge base info on what qualifies as high. It says: “Emily’s blood pressure readings have been in the prehypertensive range. For example, on Aug 20, 2025 it was 135/88, and on July 15, 130/85. These are slightly above normal.” (It derived “prehypertensive” from knowledge base or internal logic, using the data retrieved.)
	•	Satisfied, Dr. Alice says “thank you,” and the assistant politely ends the session or says it’s happy to help. The doctor can then move on to the next patient or ask further questions.

This scenario shows how the Nova Sonic voice assistant makes it easy to retrieve and understand patient data without tedious keyword search or clicking through an EHR interface. It demonstrates the power of combining voice AI with structured medical data: the doctor gets immediate answers and context, using a natural conversation. Meanwhile, behind the scenes, the system ensured all information was pulled securely from the official record (FHIR) and presented accurately (thanks to the agent grounding in real data).

Conclusion

By using Amazon Nova Sonic and Bedrock Agents, we achieve a cutting-edge solution for voice-assisted medical record search. The project delivers a natural, voice-first experience for clinicians, going beyond traditional text queries and providing semantic understanding of clinical questions. Nova Sonic’s unified speech model provides human-like, real-time dialogues with low latency ￼. The integration of Bedrock Agents allows the AI to reliably fetch patient-specific information via FHIR APIs, rather than guessing, which ensures accuracy and trustworthiness in responses ￼. AWS HealthLake (or the FHIR store) serves as a robust backbone, indexing data so it’s queryable in seconds ￼ and ensuring compliance and security of sensitive health data. The web-based UI, being static and lightweight, makes the solution easily accessible on existing devices with minimal setup, and can be deployed at scale (using S3/CloudFront) with global availability.

In essence, this implementation plan lays out how to build a voice-enabled FHIR search system that can act as a doctor’s assistant – answering questions about patient history, labs, vitals, and more, all through a convenient conversational interface. By leveraging the latest AWS generative AI services (Nova Sonic, Bedrock Agents, Knowledge Bases) alongside healthcare APIs, the product stands to significantly improve clinical workflows. Doctors can retrieve critical information in a hands-free manner, allowing them to focus more on patient care rather than paperwork or computer screens. The combination of traditional search (precise retrieval of known fields) and semantic search (understanding intent and context) makes the system flexible and powerful. With further enhancements and rigorous testing, this Nova Sonic-powered FHIR voice assistant could be piloted in a hospital setting, potentially transforming how clinicians interact with electronic health records and paving the way for more intuitive healthcare IT solutions.

Sources:
	•	Amazon Nova Sonic announcement – unified speech model with integrated understanding & generation ￼ ￼
	•	Nova Sonic real-time voice conversation and streaming API (bidirectional) ￼
	•	AWS Bedrock Agent capabilities – multi-step tool orchestration and data retrieval ￼ ￼
	•	AWS HealthLake – FHIR data store using S3, indexed for search and quick retrieval of patient data ￼ ￼xw